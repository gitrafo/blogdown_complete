---
title: "How to model using caret package - cookbook"
author: "Petr Schönbauer"
date: "2017-11-10"
slug: how-to-model-using-caret-package-cookbook
categories: []
tags:
  - R
  - model
  - glmnet
  - random forest
output:
  blogdown::html_page:
    toc: true
    fig_width: 6
    dev: "svg"
---


<div id="TOC">
<ul>
<li><a href="#data-wrangling">Data wrangling</a><ul>
<li><a href="#load-data-remove-redundat-variables">Load data, remove redundat variables</a></li>
<li><a href="#summary-of-data">Summary of data</a></li>
</ul></li>
<li><a href="#models">Models</a><ul>
<li><a href="#create-common-validation-indicies-traincontrol">Create common validation indicies, <code>trainControl</code></a></li>
<li><a href="#glmnet-model">glmnet model</a></li>
<li><a href="#randomforest-model">randomForest model</a></li>
<li><a href="#compare-glmnet-and-randomforest-performance">Compare glmnet and randomForest performance</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<p>Hello. Purpose of this post is to share and learn basics of modeling, sort of a simple “cookbook” how to apply several type of models (glmnet and random forest in this case) on the same train/test splits using caret package, evaluate and compare its performance. Some sort of reference. We´re going to predict breast cancer diagnosis (malign “M” or benign "B) from Kaggle´s <a href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data">data set</a>. Let´s get started:)</p>
<pre class="r"><code># load libraries
library(tidyverse)
library(glmnet)
library(ranger)
library(caret)
library(caTools)</code></pre>
<div id="data-wrangling" class="section level1">
<h1>Data wrangling</h1>
<div id="load-data-remove-redundat-variables" class="section level2">
<h2>Load data, remove redundat variables</h2>
<pre class="r"><code># load data
df &lt;- read_csv(&quot;../../static/data/breast_data.csv&quot;)

# glimpse raw data
df %&gt;% glimpse</code></pre>
<pre><code>## Observations: 569
## Variables: 33
## $ id                      &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 84...
## $ diagnosis               &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;...
## $ radius_mean             &lt;dbl&gt; 17.990, 20.570, 19.690, 11.420, 20.290...
## $ texture_mean            &lt;dbl&gt; 10.38, 17.77, 21.25, 20.38, 14.34, 15....
## $ perimeter_mean          &lt;dbl&gt; 122.80, 132.90, 130.00, 77.58, 135.10,...
## $ area_mean               &lt;dbl&gt; 1001.0, 1326.0, 1203.0, 386.1, 1297.0,...
## $ smoothness_mean         &lt;dbl&gt; 0.11840, 0.08474, 0.10960, 0.14250, 0....
## $ compactness_mean        &lt;dbl&gt; 0.27760, 0.07864, 0.15990, 0.28390, 0....
## $ concavity_mean          &lt;dbl&gt; 0.30010, 0.08690, 0.19740, 0.24140, 0....
## $ `concave points_mean`   &lt;dbl&gt; 0.14710, 0.07017, 0.12790, 0.10520, 0....
## $ symmetry_mean           &lt;dbl&gt; 0.2419, 0.1812, 0.2069, 0.2597, 0.1809...
## $ fractal_dimension_mean  &lt;dbl&gt; 0.07871, 0.05667, 0.05999, 0.09744, 0....
## $ radius_se               &lt;dbl&gt; 1.0950, 0.5435, 0.7456, 0.4956, 0.7572...
## $ texture_se              &lt;dbl&gt; 0.9053, 0.7339, 0.7869, 1.1560, 0.7813...
## $ perimeter_se            &lt;dbl&gt; 8.589, 3.398, 4.585, 3.445, 5.438, 2.2...
## $ area_se                 &lt;dbl&gt; 153.40, 74.08, 94.03, 27.23, 94.44, 27...
## $ smoothness_se           &lt;dbl&gt; 0.006399, 0.005225, 0.006150, 0.009110...
## $ compactness_se          &lt;dbl&gt; 0.049040, 0.013080, 0.040060, 0.074580...
## $ concavity_se            &lt;dbl&gt; 0.05373, 0.01860, 0.03832, 0.05661, 0....
## $ `concave points_se`     &lt;dbl&gt; 0.015870, 0.013400, 0.020580, 0.018670...
## $ symmetry_se             &lt;dbl&gt; 0.03003, 0.01389, 0.02250, 0.05963, 0....
## $ fractal_dimension_se    &lt;dbl&gt; 0.006193, 0.003532, 0.004571, 0.009208...
## $ radius_worst            &lt;dbl&gt; 25.38, 24.99, 23.57, 14.91, 22.54, 15....
## $ texture_worst           &lt;dbl&gt; 17.33, 23.41, 25.53, 26.50, 16.67, 23....
## $ perimeter_worst         &lt;dbl&gt; 184.60, 158.80, 152.50, 98.87, 152.20,...
## $ area_worst              &lt;dbl&gt; 2019.0, 1956.0, 1709.0, 567.7, 1575.0,...
## $ smoothness_worst        &lt;dbl&gt; 0.1622, 0.1238, 0.1444, 0.2098, 0.1374...
## $ compactness_worst       &lt;dbl&gt; 0.6656, 0.1866, 0.4245, 0.8663, 0.2050...
## $ concavity_worst         &lt;dbl&gt; 0.71190, 0.24160, 0.45040, 0.68690, 0....
## $ `concave points_worst`  &lt;dbl&gt; 0.26540, 0.18600, 0.24300, 0.25750, 0....
## $ symmetry_worst          &lt;dbl&gt; 0.4601, 0.2750, 0.3613, 0.6638, 0.2364...
## $ fractal_dimension_worst &lt;dbl&gt; 0.11890, 0.08902, 0.08758, 0.17300, 0....
## $ X33                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...</code></pre>
<pre class="r"><code># remove redundat columns
df &lt;- df %&gt;% 
  select(-id, -X33)

# convert diagnosis as factor
df$diagnosis &lt;- as.factor(df$diagnosis)</code></pre>
</div>
<div id="summary-of-data" class="section level2">
<h2>Summary of data</h2>
<pre class="r"><code># propotion of benign/malignant
table(df$diagnosis)</code></pre>
<pre><code>## 
##   B   M 
## 357 212</code></pre>
<pre class="r"><code>prop.table(table(df$diagnosis))</code></pre>
<pre><code>## 
##         B         M 
## 0.6274165 0.3725835</code></pre>
<pre class="r"><code># are there any missing values?
any(is.na(df))</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>In total:</p>
<ul>
<li>569 observations
<ul>
<li>357 benign (62%)</li>
<li>212 malignant (37%)</li>
</ul></li>
<li>30 predictor variables</li>
<li>no missing values</li>
</ul>
</div>
</div>
<div id="models" class="section level1">
<h1>Models</h1>
<div id="create-common-validation-indicies-traincontrol" class="section level2">
<h2>Create common validation indicies, <code>trainControl</code></h2>
<p>Start with dividing predictors (features) and response (class) into separated variables <code>df_x</code> and <code>df_y</code>.</p>
<pre class="r"><code># subset predictors variables
df_x &lt;- df %&gt;% select(-diagnosis)

# subset response variable
df_y &lt;- df$diagnosis</code></pre>
<p>In order to achieve fair comparison of models we have to train and test models on the same train/test splits. <code>createFolds</code> seems to be covinient way how to achieve such splits. The outcome (indices) are used later as an <code>index</code> parameter of common <code>trainControl</code> object for all tested models.</p>
<pre class="r"><code># create indices for each fold
my_folds &lt;- createFolds(df_y, k = 10)</code></pre>
<p><code>my_folds</code> now contains indices of held-out (validation) samples of each fold. In addition distribution of class (benign / malignant) is preserved.</p>
<pre class="r"><code># structure of &quot;my_fold&quot;
my_folds %&gt;% glimpse</code></pre>
<pre><code>## List of 10
##  $ Fold01: int [1:57] 19 36 46 55 62 83 88 91 101 102 ...
##  $ Fold02: int [1:57] 1 16 21 29 32 45 47 53 66 69 ...
##  $ Fold03: int [1:57] 9 11 35 37 50 51 54 56 72 74 ...
##  $ Fold04: int [1:57] 8 25 34 44 71 90 116 122 124 128 ...
##  $ Fold05: int [1:57] 2 5 7 20 23 59 63 67 75 76 ...
##  $ Fold06: int [1:57] 26 27 30 33 57 65 78 85 87 97 ...
##  $ Fold07: int [1:58] 4 12 17 18 49 52 61 68 82 98 ...
##  $ Fold08: int [1:56] 3 10 15 31 42 43 48 58 60 92 ...
##  $ Fold09: int [1:57] 6 22 28 38 39 40 41 64 140 143 ...
##  $ Fold10: int [1:56] 13 14 24 81 94 96 109 114 123 139 ...</code></pre>
<pre class="r"><code># distribution of class is preserved in each fold
my_folds %&gt;% 
  map_df(~prop.table(table(df_y[.])))</code></pre>
<pre><code>## # A tibble: 2 x 10
##   Fold01 Fold02 Fold03 Fold04 Fold05 Fold06 Fold07 Fold08 Fold09 Fold10
##    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1  0.632  0.632  0.614  0.632  0.632  0.632  0.621  0.625  0.632  0.625
## 2  0.368  0.368  0.386  0.368  0.368  0.368  0.379  0.375  0.368  0.375</code></pre>
<pre class="r"><code># create trainControl object
my_trainControl &lt;- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = FALSE,
  savePredictions = TRUE,
  index = my_folds)</code></pre>
</div>
<div id="glmnet-model" class="section level2">
<h2>glmnet model</h2>
<pre class="r"><code># train glmnet model
model_glmnet &lt;- train(x = df_x, y = df_y, 
                   method = &quot;glmnet&quot;,
                   metric = &quot;ROC&quot;,
                   tuneLength = 3,
                   trControl = my_trainControl)

# print model
model_glmnet</code></pre>
<pre><code>## glmnet 
## 
## 569 samples
##  30 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (10 reps) 
## Summary of sample sizes: 57, 57, 57, 57, 57, 57, ... 
## Resampling results across tuning parameters:
## 
##   alpha  lambda        ROC        Sens       Spec     
##   0.10   0.0007673665  0.9886649  0.9744906  0.9172224
##   0.10   0.0076736649  0.9913538  0.9810182  0.9198374
##   0.10   0.0767366489  0.9925542  0.9903514  0.8988619
##   0.55   0.0007673665  0.9874316  0.9716888  0.9130256
##   0.55   0.0076736649  0.9906126  0.9807018  0.9135492
##   0.55   0.0767366489  0.9905134  0.9903533  0.8773684
##   1.00   0.0007673665  0.9848590  0.9654602  0.9098650
##   1.00   0.0076736649  0.9892984  0.9775923  0.9088151
##   1.00   0.0767366489  0.9857073  0.9859968  0.8579884
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were alpha = 0.1 and lambda
##  = 0.07673665.</code></pre>
<p>Some remarks:</p>
<ul>
<li>ROC = area under curve</li>
<li>Sens = sensitivity = true positive rate = proportion of positives that are correctly identified as such</li>
<li>Spec = specificity = true negative rate = proportion of negatives that are correctly identified as such</li>
</ul>
<p>Another approach to find the model with the highest ROC:</p>
<pre class="r"><code>model_glmnet$results %&gt;% 
  as.tibble %&gt;%  
  arrange(desc(ROC))</code></pre>
<pre><code>## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.</code></pre>
<pre><code>## # A tibble: 9 x 8
##   alpha   lambda   ROC  Sens  Spec   ROCSD  SensSD SpecSD
##   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1  0.1  0.0767   0.993 0.990 0.899 0.00219 0.00680 0.0417
## 2  0.1  0.00767  0.991 0.981 0.920 0.00286 0.0162  0.0430
## 3  0.55 0.00767  0.991 0.981 0.914 0.00295 0.0163  0.0437
## 4  0.55 0.0767   0.991 0.990 0.877 0.00303 0.00849 0.0409
## 5  1    0.00767  0.989 0.978 0.909 0.00363 0.0194  0.0413
## 6  0.1  0.000767 0.989 0.974 0.917 0.00633 0.0225  0.0494
## 7  0.55 0.000767 0.987 0.972 0.913 0.00690 0.0245  0.0516
## 8  1    0.0767   0.986 0.986 0.858 0.00439 0.0137  0.0403
## 9  1    0.000767 0.985 0.965 0.910 0.00852 0.0283  0.0533</code></pre>
<pre class="r"><code># plot ROCs
plot(model_glmnet)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-10-1.svg" width="576" /></p>
<p>By default, the tuning parameters <code>alpha</code> and <code>lamda</code> have three values (<code>tuneLength = 3</code> in <code>train</code> function). Let´s try 20.</p>
<pre class="r"><code># train glmnet model with tuneLength = 10
model_glmnet &lt;- train(x = df_x, y = df_y, 
                   method = &quot;glmnet&quot;,
                   metric = &quot;ROC&quot;,
                   tuneLength = 20,
                   trControl = my_trainControl)</code></pre>
<pre class="r"><code>model_glmnet$results %&gt;% 
  as.tibble %&gt;%  
  arrange(desc(ROC))</code></pre>
<pre><code>## # A tibble: 400 x 8
##    alpha lambda   ROC  Sens  Spec   ROCSD  SensSD SpecSD
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
##  1 0.1   0.0552 0.993 0.991 0.908 0.00200 0.00726 0.0417
##  2 0.147 0.0552 0.993 0.991 0.908 0.00216 0.00749 0.0417
##  3 0.1   0.0356 0.993 0.987 0.913 0.00183 0.0111  0.0424
##  4 0.1   0.0856 0.993 0.991 0.898 0.00228 0.00619 0.0406
##  5 0.147 0.0356 0.992 0.988 0.914 0.00192 0.0114  0.0403
##  6 0.195 0.0552 0.992 0.990 0.907 0.00230 0.00680 0.0429
##  7 0.195 0.0356 0.992 0.988 0.911 0.00200 0.0106  0.0426
##  8 0.147 0.0856 0.992 0.992 0.896 0.00236 0.00640 0.0412
##  9 0.242 0.0356 0.992 0.988 0.910 0.00212 0.0125  0.0430
## 10 0.242 0.0552 0.992 0.990 0.906 0.00240 0.00854 0.0447
## # ... with 390 more rows</code></pre>
<p>Tha value ~0.990 seems to be the peak we can get without additional tuning.</p>
<pre class="r"><code># plot ROCs
plot(model_glmnet)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-13-1.svg" width="576" /></p>
</div>
<div id="randomforest-model" class="section level2">
<h2>randomForest model</h2>
<pre class="r"><code># train randomForest model
model_randomForest &lt;- train(x = df_x, y = df_y, 
                            method = &quot;ranger&quot;,
                            metric = &quot;ROC&quot;,
                            importance = &quot;permutation&quot;,
                            tuneLength = 10,
                            trControl = my_trainControl)

# print model
model_randomForest</code></pre>
<pre><code>## Random Forest 
## 
## 569 samples
##  30 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (10 reps) 
## Summary of sample sizes: 57, 57, 57, 57, 57, 57, ... 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   ROC        Sens       Spec     
##    2    gini        0.9875692  0.9688744  0.8941499
##    2    extratrees  0.9896054  0.9807028  0.8936401
##    5    gini        0.9861957  0.9595364  0.8931138
##    5    extratrees  0.9899611  0.9760366  0.9051584
##    8    gini        0.9857608  0.9520743  0.8978314
##    8    extratrees  0.9898579  0.9722983  0.9051529
##   11    gini        0.9854198  0.9542462  0.8973050
##   11    extratrees  0.9894603  0.9719887  0.9046294
##   14    gini        0.9848390  0.9502032  0.9009810
##   14    extratrees  0.9893051  0.9701196  0.9088234
##   17    gini        0.9850303  0.9526925  0.9025572
##   17    extratrees  0.9891023  0.9685648  0.9093469
##   20    gini        0.9848963  0.9505215  0.9062193
##   20    extratrees  0.9891365  0.9691860  0.9098732
##   23    gini        0.9845895  0.9520723  0.9030697
##   23    extratrees  0.9889583  0.9676293  0.9124938
##   26    gini        0.9849933  0.9489580  0.9072637
##   26    extratrees  0.9886160  0.9676312  0.9130201
##   30    gini        0.9842421  0.9464678  0.9062138
##   30    extratrees  0.9885429  0.9670101  0.9140783
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 5, splitrule =
##  extratrees and min.node.size = 1.</code></pre>
<pre class="r"><code># plot model
plot(model_randomForest)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-14-1.svg" width="576" /></p>
<pre class="r"><code>model_randomForest$results %&gt;% 
  as.tibble %&gt;%  
  arrange(desc(ROC))</code></pre>
<pre><code>## # A tibble: 20 x 9
##     mtry min.node.size splitrule    ROC  Sens  Spec   ROCSD SensSD SpecSD
##    &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1     5             1 extratrees 0.990 0.976 0.905 0.00276 0.0124 0.0326
##  2     8             1 extratrees 0.990 0.972 0.905 0.00255 0.0138 0.0360
##  3     2             1 extratrees 0.990 0.981 0.894 0.00280 0.0107 0.0291
##  4    11             1 extratrees 0.989 0.972 0.905 0.00289 0.0132 0.0360
##  5    14             1 extratrees 0.989 0.970 0.909 0.00306 0.0130 0.0342
##  6    20             1 extratrees 0.989 0.969 0.910 0.00364 0.0151 0.0369
##  7    17             1 extratrees 0.989 0.969 0.909 0.00356 0.0160 0.0364
##  8    23             1 extratrees 0.989 0.968 0.912 0.00337 0.0139 0.0351
##  9    26             1 extratrees 0.989 0.968 0.913 0.00359 0.0156 0.0351
## 10    30             1 extratrees 0.989 0.967 0.914 0.00322 0.0160 0.0379
## 11     2             1 gini       0.988 0.969 0.894 0.00324 0.0154 0.0348
## 12     5             1 gini       0.986 0.960 0.893 0.00345 0.0185 0.0372
## 13     8             1 gini       0.986 0.952 0.898 0.00379 0.0213 0.0369
## 14    11             1 gini       0.985 0.954 0.897 0.00427 0.0218 0.0373
## 15    17             1 gini       0.985 0.953 0.903 0.00406 0.0265 0.0407
## 16    26             1 gini       0.985 0.949 0.907 0.00407 0.0276 0.0349
## 17    20             1 gini       0.985 0.951 0.906 0.00441 0.0243 0.0352
## 18    14             1 gini       0.985 0.950 0.901 0.00384 0.0255 0.0359
## 19    23             1 gini       0.985 0.952 0.903 0.00459 0.0224 0.0352
## 20    30             1 gini       0.984 0.946 0.906 0.00425 0.0260 0.0329</code></pre>
<p>Now we can compare top 10 importance variables:</p>
<pre class="r"><code>plot(varImp(model_glmnet, scale = FALSE), top = 10, main = &quot;glmnet&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-16-1.svg" width="576" /></p>
<pre class="r"><code>plot(varImp(model_randomForest, scale = FALSE), top = 10, main = &quot;randomForest&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-16-2.svg" width="576" /></p>
<p>Suprisingly variables quite differs for both models. Any hint why is welcomed:)</p>
</div>
<div id="compare-glmnet-and-randomforest-performance" class="section level2">
<h2>Compare glmnet and randomForest performance</h2>
<pre class="r"><code>model_list &lt;- list(glmnet = model_glmnet, randomForest = model_randomForest)
resamples &lt;- resamples(model_list)

# plot the comparison
bwplot(resamples, metric = &quot;ROC&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-17-1.svg" width="576" /></p>
<pre class="r"><code># plot the comparison per each fold
xyplot(resamples, metric = &quot;ROC&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-17-2.svg" width="576" /></p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<ul>
<li>the glmnet seems to fit the data better than randomForest</li>
<li>even without advanced tuning pretty decent level of accuracy can be achieved</li>
<li>open points: difference between variable importance of both models</li>
</ul>
<p>Thanks for reading. Sure, there is a lot of space for improvements, tweaks. Any kind of feedback is appreciated.</p>
</div>
