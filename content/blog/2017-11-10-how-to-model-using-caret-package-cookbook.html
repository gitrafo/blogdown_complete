---
title: "How to model using caret package - cookbook"
author: "Petr Schönbauer"
date: "2017-11-10"
slug: how-to-model-using-caret-package-cookbook
categories: []
tags:
  - R
  - model
  - glmnet
  - random forest
output:
  blogdown::html_page:
    toc: true
    fig_width: 6
    dev: "svg"
---


<div id="TOC">
<ul>
<li><a href="#data-wrangling">Data wrangling</a><ul>
<li><a href="#load-data-remove-redundat-variables">Load data, remove redundat variables</a></li>
<li><a href="#summary-of-data">Summary of data</a></li>
</ul></li>
<li><a href="#models">Models</a><ul>
<li><a href="#create-common-validation-indicies-traincontrol">Create common validation indicies, <code>trainControl</code></a></li>
<li><a href="#glmnet-model">glmnet model</a></li>
<li><a href="#randomforest-model">randomForest model</a></li>
<li><a href="#compare-glmnet-and-randomforest-performance">Compare glmnet and randomForest performance</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<p>Hello. Purpose of this post is to share and learn basics of modeling, sort of a simple “cookbook” how to apply several type of models (glmnet and random forest in this case) on the same train/test splits using caret package, evaluate and compare its performance. Some sort of reference. We´re going to predict breast cancer diagnosis (malign “M” or benign "B) from Kaggle´s <a href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data">data set</a>. Let´s get started:)</p>
<pre class="r"><code># load libraries
library(tidyverse)
library(glmnet)
library(ranger)
library(caret)
library(caTools)</code></pre>
<div id="data-wrangling" class="section level1">
<h1>Data wrangling</h1>
<div id="load-data-remove-redundat-variables" class="section level2">
<h2>Load data, remove redundat variables</h2>
<pre class="r"><code># load data
df &lt;- read_csv(&quot;../../static/data/breast_data.csv&quot;)

# glimpse raw data
df %&gt;% glimpse</code></pre>
<pre><code>## Observations: 569
## Variables: 33
## $ id                      &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 84...
## $ diagnosis               &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;...
## $ radius_mean             &lt;dbl&gt; 17.990, 20.570, 19.690, 11.420, 20.290...
## $ texture_mean            &lt;dbl&gt; 10.38, 17.77, 21.25, 20.38, 14.34, 15....
## $ perimeter_mean          &lt;dbl&gt; 122.80, 132.90, 130.00, 77.58, 135.10,...
## $ area_mean               &lt;dbl&gt; 1001.0, 1326.0, 1203.0, 386.1, 1297.0,...
## $ smoothness_mean         &lt;dbl&gt; 0.11840, 0.08474, 0.10960, 0.14250, 0....
## $ compactness_mean        &lt;dbl&gt; 0.27760, 0.07864, 0.15990, 0.28390, 0....
## $ concavity_mean          &lt;dbl&gt; 0.30010, 0.08690, 0.19740, 0.24140, 0....
## $ `concave points_mean`   &lt;dbl&gt; 0.14710, 0.07017, 0.12790, 0.10520, 0....
## $ symmetry_mean           &lt;dbl&gt; 0.2419, 0.1812, 0.2069, 0.2597, 0.1809...
## $ fractal_dimension_mean  &lt;dbl&gt; 0.07871, 0.05667, 0.05999, 0.09744, 0....
## $ radius_se               &lt;dbl&gt; 1.0950, 0.5435, 0.7456, 0.4956, 0.7572...
## $ texture_se              &lt;dbl&gt; 0.9053, 0.7339, 0.7869, 1.1560, 0.7813...
## $ perimeter_se            &lt;dbl&gt; 8.589, 3.398, 4.585, 3.445, 5.438, 2.2...
## $ area_se                 &lt;dbl&gt; 153.40, 74.08, 94.03, 27.23, 94.44, 27...
## $ smoothness_se           &lt;dbl&gt; 0.006399, 0.005225, 0.006150, 0.009110...
## $ compactness_se          &lt;dbl&gt; 0.049040, 0.013080, 0.040060, 0.074580...
## $ concavity_se            &lt;dbl&gt; 0.05373, 0.01860, 0.03832, 0.05661, 0....
## $ `concave points_se`     &lt;dbl&gt; 0.015870, 0.013400, 0.020580, 0.018670...
## $ symmetry_se             &lt;dbl&gt; 0.03003, 0.01389, 0.02250, 0.05963, 0....
## $ fractal_dimension_se    &lt;dbl&gt; 0.006193, 0.003532, 0.004571, 0.009208...
## $ radius_worst            &lt;dbl&gt; 25.38, 24.99, 23.57, 14.91, 22.54, 15....
## $ texture_worst           &lt;dbl&gt; 17.33, 23.41, 25.53, 26.50, 16.67, 23....
## $ perimeter_worst         &lt;dbl&gt; 184.60, 158.80, 152.50, 98.87, 152.20,...
## $ area_worst              &lt;dbl&gt; 2019.0, 1956.0, 1709.0, 567.7, 1575.0,...
## $ smoothness_worst        &lt;dbl&gt; 0.1622, 0.1238, 0.1444, 0.2098, 0.1374...
## $ compactness_worst       &lt;dbl&gt; 0.6656, 0.1866, 0.4245, 0.8663, 0.2050...
## $ concavity_worst         &lt;dbl&gt; 0.71190, 0.24160, 0.45040, 0.68690, 0....
## $ `concave points_worst`  &lt;dbl&gt; 0.26540, 0.18600, 0.24300, 0.25750, 0....
## $ symmetry_worst          &lt;dbl&gt; 0.4601, 0.2750, 0.3613, 0.6638, 0.2364...
## $ fractal_dimension_worst &lt;dbl&gt; 0.11890, 0.08902, 0.08758, 0.17300, 0....
## $ X33                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...</code></pre>
<pre class="r"><code># remove redundat columns
df &lt;- df %&gt;% 
  select(-id, -X33)

# convert diagnosis as factor
df$diagnosis &lt;- as.factor(df$diagnosis)</code></pre>
</div>
<div id="summary-of-data" class="section level2">
<h2>Summary of data</h2>
<pre class="r"><code># propotion of benign/malignant
table(df$diagnosis)</code></pre>
<pre><code>## 
##   B   M 
## 357 212</code></pre>
<pre class="r"><code>prop.table(table(df$diagnosis))</code></pre>
<pre><code>## 
##         B         M 
## 0.6274165 0.3725835</code></pre>
<pre class="r"><code># are there any missing values?
any(is.na(df))</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>In total:</p>
<ul>
<li>569 observations
<ul>
<li>357 benign (62%)</li>
<li>212 malignant (37%)</li>
</ul></li>
<li>30 predictor variables</li>
<li>no missing values</li>
</ul>
</div>
</div>
<div id="models" class="section level1">
<h1>Models</h1>
<div id="create-common-validation-indicies-traincontrol" class="section level2">
<h2>Create common validation indicies, <code>trainControl</code></h2>
<p>Start with dividing predictors (features) and response (class) into separated variables <code>df_x</code> and <code>df_y</code>.</p>
<pre class="r"><code># subset predictors variables
df_x &lt;- df %&gt;% select(-diagnosis)

# subset response variable
df_y &lt;- df$diagnosis</code></pre>
<p>In order to achieve fair comparison of models we have to train and test models on the same train/test splits. <code>createFolds</code> seems to be covinient way how to achieve such splits. The outcome (indices) are used later as an <code>index</code> parameter of common <code>trainControl</code> object for all tested models.</p>
<pre class="r"><code># create indices for each fold
my_folds &lt;- createFolds(df_y, k = 10)</code></pre>
<p><code>my_folds</code> now contains indices of held-out (validation) samples of each fold. In addition distribution of class (benign / malignant) is preserved.</p>
<pre class="r"><code># structure of &quot;my_fold&quot;
my_folds %&gt;% glimpse</code></pre>
<pre><code>## List of 10
##  $ Fold01: int [1:57] 4 19 21 23 24 25 26 36 47 48 ...
##  $ Fold02: int [1:57] 3 28 29 35 45 54 61 90 99 103 ...
##  $ Fold03: int [1:56] 11 31 49 57 64 83 85 114 120 130 ...
##  $ Fold04: int [1:57] 6 10 18 33 55 67 75 79 81 88 ...
##  $ Fold05: int [1:57] 5 9 34 42 66 72 74 84 93 97 ...
##  $ Fold06: int [1:56] 30 50 53 62 80 89 95 101 108 117 ...
##  $ Fold07: int [1:58] 8 16 20 22 27 40 44 58 59 65 ...
##  $ Fold08: int [1:57] 1 2 7 13 17 32 46 63 68 69 ...
##  $ Fold09: int [1:57] 12 14 15 37 39 43 51 52 60 73 ...
##  $ Fold10: int [1:57] 38 41 92 106 116 129 136 151 157 158 ...</code></pre>
<pre class="r"><code># distribution of class is preserved in each fold
my_folds %&gt;% 
  map_df(~prop.table(table(df_y[.])))</code></pre>
<pre><code>## # A tibble: 2 x 10
##   Fold01 Fold02 Fold03 Fold04 Fold05 Fold06 Fold07 Fold08 Fold09 Fold10
##    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1  0.632  0.632  0.625  0.632  0.632  0.625  0.621  0.614  0.632  0.632
## 2  0.368  0.368  0.375  0.368  0.368  0.375  0.379  0.386  0.368  0.368</code></pre>
<pre class="r"><code># create trainControl object
my_trainControl &lt;- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = FALSE,
  savePredictions = TRUE,
  index = my_folds)</code></pre>
</div>
<div id="glmnet-model" class="section level2">
<h2>glmnet model</h2>
<pre class="r"><code># train glmnet model
model_glmnet &lt;- train(x = df_x, y = df_y, 
                   method = &quot;glmnet&quot;,
                   metric = &quot;ROC&quot;,
                   tuneLength = 3,
                   trControl = my_trainControl)

# print model
model_glmnet</code></pre>
<pre><code>## glmnet 
## 
## 569 samples
##  30 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (10 reps) 
## Summary of sample sizes: 57, 57, 56, 57, 57, 56, ... 
## Resampling results across tuning parameters:
## 
##   alpha  lambda        ROC        Sens       Spec     
##   0.10   0.0007673665  0.9855982  0.9601633  0.9187517
##   0.10   0.0076736649  0.9877926  0.9704311  0.9198044
##   0.10   0.0767366489  0.9897785  0.9831930  0.8956958
##   0.55   0.0007673665  0.9846794  0.9561173  0.9213640
##   0.55   0.0076736649  0.9865223  0.9663851  0.9203224
##   0.55   0.0767366489  0.9872521  0.9844411  0.8721190
##   1.00   0.0007673665  0.9826728  0.9520685  0.9208377
##   1.00   0.0076736649  0.9837383  0.9614075  0.9150896
##   1.00   0.0767366489  0.9813845  0.9847536  0.8469661
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were alpha = 0.1 and lambda
##  = 0.07673665.</code></pre>
<p>Some remarks:</p>
<ul>
<li>ROC = area under curve</li>
<li>Sens = sensitivity = true positive rate = proportion of positives that are correctly identified as such</li>
<li>Spec = specificity = true negative rate = proportion of negatives that are correctly identified as such</li>
</ul>
<p>Another approach to find the model with the highest ROC:</p>
<pre class="r"><code>model_glmnet$results %&gt;% 
  as.tibble %&gt;%  
  arrange(desc(ROC))</code></pre>
<pre><code>## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.</code></pre>
<pre><code>## # A tibble: 9 x 8
##   alpha   lambda   ROC  Sens  Spec   ROCSD SensSD SpecSD
##   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1  0.1  0.0767   0.990 0.983 0.896 0.00496 0.0136 0.0520
## 2  0.1  0.00767  0.988 0.970 0.920 0.00639 0.0263 0.0481
## 3  0.55 0.0767   0.987 0.984 0.872 0.00683 0.0131 0.0638
## 4  0.55 0.00767  0.987 0.966 0.920 0.00780 0.0260 0.0465
## 5  0.1  0.000767 0.986 0.960 0.919 0.00773 0.0348 0.0462
## 6  0.55 0.000767 0.985 0.956 0.921 0.00891 0.0345 0.0452
## 7  1    0.00767  0.984 0.961 0.915 0.00975 0.0267 0.0491
## 8  1    0.000767 0.983 0.952 0.921 0.0121  0.0376 0.0434
## 9  1    0.0767   0.981 0.985 0.847 0.0171  0.0129 0.0769</code></pre>
<pre class="r"><code># plot ROCs
plot(model_glmnet)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-10-1.svg" width="576" /></p>
<p>By default, the tuning parameters <code>alpha</code> and <code>lamda</code> have three values (<code>tuneLength = 3</code> in <code>train</code> function). Let´s try 20.</p>
<pre class="r"><code># train glmnet model with tuneLength = 10
model_glmnet &lt;- train(x = df_x, y = df_y, 
                   method = &quot;glmnet&quot;,
                   metric = &quot;ROC&quot;,
                   tuneLength = 20,
                   trControl = my_trainControl)</code></pre>
<pre class="r"><code>model_glmnet$results %&gt;% 
  as.tibble %&gt;%  
  arrange(desc(ROC))</code></pre>
<pre><code>## # A tibble: 400 x 8
##    alpha lambda   ROC  Sens  Spec   ROCSD  SensSD SpecSD
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
##  1 0.1   0.0856 0.990 0.984 0.893 0.00494 0.0134  0.0517
##  2 0.1   0.0552 0.990 0.981 0.905 0.00513 0.0164  0.0559
##  3 0.147 0.0856 0.990 0.984 0.890 0.00496 0.0116  0.0533
##  4 0.147 0.0552 0.989 0.981 0.902 0.00526 0.0153  0.0549
##  5 0.1   0.133  0.989 0.986 0.882 0.00470 0.0103  0.0503
##  6 0.1   0.0356 0.989 0.979 0.914 0.00540 0.0158  0.0498
##  7 0.147 0.0356 0.989 0.979 0.912 0.00555 0.0158  0.0512
##  8 0.195 0.0552 0.989 0.981 0.899 0.00540 0.0139  0.0553
##  9 0.147 0.133  0.989 0.987 0.877 0.00471 0.00978 0.0482
## 10 0.195 0.0856 0.989 0.985 0.890 0.00512 0.0108  0.0516
## # ... with 390 more rows</code></pre>
<p>Tha value ~0.990 seems to be the peak we can get without additional tuning.</p>
<pre class="r"><code># plot ROCs
plot(model_glmnet)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-13-1.svg" width="576" /></p>
</div>
<div id="randomforest-model" class="section level2">
<h2>randomForest model</h2>
<pre class="r"><code># train randomForest model
model_randomForest &lt;- train(x = df_x, y = df_y, 
                            method = &quot;ranger&quot;,
                            metric = &quot;ROC&quot;,
                            importance = &quot;permutation&quot;,
                            tuneLength = 10,
                            trControl = my_trainControl)

# print model
model_randomForest</code></pre>
<pre><code>## Random Forest 
## 
## 569 samples
##  30 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (10 reps) 
## Summary of sample sizes: 57, 57, 56, 57, 57, 56, ... 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   ROC        Sens       Spec     
##    2    gini        0.9865905  0.9654583  0.8888399
##    2    extratrees  0.9890253  0.9779058  0.8846680
##    5    gini        0.9841448  0.9589220  0.8977514
##    5    extratrees  0.9888530  0.9710600  0.8930422
##    8    gini        0.9830423  0.9586115  0.8898953
##    8    extratrees  0.9879186  0.9679495  0.8951309
##   11    gini        0.9825320  0.9586105  0.8935630
##   11    extratrees  0.9880019  0.9667025  0.9008983
##   14    gini        0.9816081  0.9561154  0.8919840
##   14    extratrees  0.9873456  0.9645276  0.8982805
##   17    gini        0.9805044  0.9570509  0.8904133
##   17    extratrees  0.9869037  0.9632824  0.9024718
##   20    gini        0.9805083  0.9567365  0.8883108
##   20    extratrees  0.9868372  0.9648381  0.8977570
##   23    gini        0.9795626  0.9520675  0.8862138
##   23    extratrees  0.9865487  0.9623479  0.9019482
##   26    gini        0.9795617  0.9526886  0.8846459
##   26    extratrees  0.9862132  0.9617229  0.8982750
##   30    gini        0.9788406  0.9530011  0.8809755
##   30    extratrees  0.9855983  0.9601681  0.9035106
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 2, splitrule =
##  extratrees and min.node.size = 1.</code></pre>
<pre class="r"><code># plot model
plot(model_randomForest)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-14-1.svg" width="576" /></p>
<pre class="r"><code>model_randomForest$results %&gt;% 
  as.tibble %&gt;%  
  arrange(desc(ROC))</code></pre>
<pre><code>## # A tibble: 20 x 9
##     mtry min.node.size splitrule    ROC  Sens  Spec   ROCSD SensSD SpecSD
##    &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1     2             1 extratrees 0.989 0.978 0.885 0.00316 0.0170 0.0374
##  2     5             1 extratrees 0.989 0.971 0.893 0.00325 0.0202 0.0432
##  3    11             1 extratrees 0.988 0.967 0.901 0.00383 0.0215 0.0490
##  4     8             1 extratrees 0.988 0.968 0.895 0.00392 0.0205 0.0535
##  5    14             1 extratrees 0.987 0.965 0.898 0.00416 0.0243 0.0494
##  6    17             1 extratrees 0.987 0.963 0.902 0.00433 0.0226 0.0490
##  7    20             1 extratrees 0.987 0.965 0.898 0.00469 0.0236 0.0502
##  8     2             1 gini       0.987 0.965 0.889 0.00366 0.0244 0.0457
##  9    23             1 extratrees 0.987 0.962 0.902 0.00528 0.0243 0.0522
## 10    26             1 extratrees 0.986 0.962 0.898 0.00600 0.0248 0.0537
## 11    30             1 extratrees 0.986 0.960 0.904 0.00608 0.0242 0.0552
## 12     5             1 gini       0.984 0.959 0.898 0.00515 0.0267 0.0501
## 13     8             1 gini       0.983 0.959 0.890 0.00653 0.0289 0.0493
## 14    11             1 gini       0.983 0.959 0.894 0.00671 0.0281 0.0519
## 15    14             1 gini       0.982 0.956 0.892 0.00730 0.0260 0.0553
## 16    20             1 gini       0.981 0.957 0.888 0.00742 0.0249 0.0580
## 17    17             1 gini       0.981 0.957 0.890 0.00825 0.0260 0.0587
## 18    23             1 gini       0.980 0.952 0.886 0.00852 0.0256 0.0617
## 19    26             1 gini       0.980 0.953 0.885 0.00795 0.0277 0.0607
## 20    30             1 gini       0.979 0.953 0.881 0.00819 0.0260 0.0633</code></pre>
<p>Now we can compare top 10 importance variables:</p>
<pre class="r"><code>plot(varImp(model_glmnet, scale = FALSE), top = 10, main = &quot;glmnet&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-16-1.svg" width="576" /></p>
<pre class="r"><code>plot(varImp(model_randomForest, scale = FALSE), top = 10, main = &quot;randomForest&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-16-2.svg" width="576" /></p>
<p>Suprisingly variables quite differs for both models. Any hint why is welcomed:)</p>
</div>
<div id="compare-glmnet-and-randomforest-performance" class="section level2">
<h2>Compare glmnet and randomForest performance</h2>
<pre class="r"><code>model_list &lt;- list(glmnet = model_glmnet, randomForest = model_randomForest)
resamples &lt;- resamples(model_list)

# plot the comparison
bwplot(resamples, metric = &quot;ROC&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-17-1.svg" width="576" /></p>
<pre class="r"><code># plot the comparison per each fold
xyplot(resamples, metric = &quot;ROC&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-17-2.svg" width="576" /></p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<ul>
<li>the glmnet seems to fit the data better than randomForest</li>
<li>even without advanced tuning pretty decent level of accuracy can be achieved</li>
<li>open points: difference between variable importance of both models</li>
</ul>
<p>Thanks for reading. Sure, there is a lot of space for improvements, tweaks. Any kind of feedback is appreciated.</p>
</div>
