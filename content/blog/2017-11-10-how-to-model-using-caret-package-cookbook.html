---
title: "How to model using caret package - cookbook"
author: "Petr Schönbauer"
date: "2017-11-10"
slug: how-to-model-using-caret-package-cookbook
categories: []
tags:
  - R
  - model
  - glmnet
  - random forest
output:
  blogdown::html_page:
    toc: true
    fig_width: 6
    dev: "svg"
---


<div id="TOC">
<ul>
<li><a href="#data-wrangling">Data wrangling</a><ul>
<li><a href="#load-data-remove-redundat-variables">Load data, remove redundat variables</a></li>
<li><a href="#summary-of-data">Summary of data</a></li>
</ul></li>
<li><a href="#models">Models</a><ul>
<li><a href="#create-common-validation-indicies-traincontrol">Create common validation indicies, <code>trainControl</code></a></li>
<li><a href="#glmnet-model">glmnet model</a></li>
<li><a href="#randomforest-model">randomForest model</a></li>
<li><a href="#compare-glmnet-and-randomforest-performance">Compare glmnet and randomForest performance</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<p>Hello. Purpose of this post is to share and learn basics of modeling, sort of a simple “cookbook” how to apply several type of models (glmnet and random forest in this case) on the same train/test splits using caret package, evaluate and compare its performance. Some sort of reference. We´re going to predict breast cancer diagnosis (malign “M” or benign "B) from Kaggle´s <a href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data">data set</a>. Let´s get started:)</p>
<pre class="r"><code># load libraries
library(tidyverse)
library(glmnet)
library(ranger)
library(caret)
library(caTools)</code></pre>
<div id="data-wrangling" class="section level1">
<h1>Data wrangling</h1>
<div id="load-data-remove-redundat-variables" class="section level2">
<h2>Load data, remove redundat variables</h2>
<pre class="r"><code># load data
df &lt;- read_csv(&quot;../../static/data/breast_data.csv&quot;)

# glimpse raw data
df %&gt;% glimpse</code></pre>
<pre><code>## Observations: 569
## Variables: 33
## $ id                      &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 84...
## $ diagnosis               &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;...
## $ radius_mean             &lt;dbl&gt; 17.990, 20.570, 19.690, 11.420, 20.290...
## $ texture_mean            &lt;dbl&gt; 10.38, 17.77, 21.25, 20.38, 14.34, 15....
## $ perimeter_mean          &lt;dbl&gt; 122.80, 132.90, 130.00, 77.58, 135.10,...
## $ area_mean               &lt;dbl&gt; 1001.0, 1326.0, 1203.0, 386.1, 1297.0,...
## $ smoothness_mean         &lt;dbl&gt; 0.11840, 0.08474, 0.10960, 0.14250, 0....
## $ compactness_mean        &lt;dbl&gt; 0.27760, 0.07864, 0.15990, 0.28390, 0....
## $ concavity_mean          &lt;dbl&gt; 0.30010, 0.08690, 0.19740, 0.24140, 0....
## $ `concave points_mean`   &lt;dbl&gt; 0.14710, 0.07017, 0.12790, 0.10520, 0....
## $ symmetry_mean           &lt;dbl&gt; 0.2419, 0.1812, 0.2069, 0.2597, 0.1809...
## $ fractal_dimension_mean  &lt;dbl&gt; 0.07871, 0.05667, 0.05999, 0.09744, 0....
## $ radius_se               &lt;dbl&gt; 1.0950, 0.5435, 0.7456, 0.4956, 0.7572...
## $ texture_se              &lt;dbl&gt; 0.9053, 0.7339, 0.7869, 1.1560, 0.7813...
## $ perimeter_se            &lt;dbl&gt; 8.589, 3.398, 4.585, 3.445, 5.438, 2.2...
## $ area_se                 &lt;dbl&gt; 153.40, 74.08, 94.03, 27.23, 94.44, 27...
## $ smoothness_se           &lt;dbl&gt; 0.006399, 0.005225, 0.006150, 0.009110...
## $ compactness_se          &lt;dbl&gt; 0.049040, 0.013080, 0.040060, 0.074580...
## $ concavity_se            &lt;dbl&gt; 0.05373, 0.01860, 0.03832, 0.05661, 0....
## $ `concave points_se`     &lt;dbl&gt; 0.015870, 0.013400, 0.020580, 0.018670...
## $ symmetry_se             &lt;dbl&gt; 0.03003, 0.01389, 0.02250, 0.05963, 0....
## $ fractal_dimension_se    &lt;dbl&gt; 0.006193, 0.003532, 0.004571, 0.009208...
## $ radius_worst            &lt;dbl&gt; 25.38, 24.99, 23.57, 14.91, 22.54, 15....
## $ texture_worst           &lt;dbl&gt; 17.33, 23.41, 25.53, 26.50, 16.67, 23....
## $ perimeter_worst         &lt;dbl&gt; 184.60, 158.80, 152.50, 98.87, 152.20,...
## $ area_worst              &lt;dbl&gt; 2019.0, 1956.0, 1709.0, 567.7, 1575.0,...
## $ smoothness_worst        &lt;dbl&gt; 0.1622, 0.1238, 0.1444, 0.2098, 0.1374...
## $ compactness_worst       &lt;dbl&gt; 0.6656, 0.1866, 0.4245, 0.8663, 0.2050...
## $ concavity_worst         &lt;dbl&gt; 0.71190, 0.24160, 0.45040, 0.68690, 0....
## $ `concave points_worst`  &lt;dbl&gt; 0.26540, 0.18600, 0.24300, 0.25750, 0....
## $ symmetry_worst          &lt;dbl&gt; 0.4601, 0.2750, 0.3613, 0.6638, 0.2364...
## $ fractal_dimension_worst &lt;dbl&gt; 0.11890, 0.08902, 0.08758, 0.17300, 0....
## $ X33                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...</code></pre>
<pre class="r"><code># remove redundat columns
df &lt;- df %&gt;% 
  select(-id, -X33)

# convert diagnosis as factor
df$diagnosis &lt;- as.factor(df$diagnosis)</code></pre>
</div>
<div id="summary-of-data" class="section level2">
<h2>Summary of data</h2>
<pre class="r"><code># propotion of benign/malignant
table(df$diagnosis)</code></pre>
<pre><code>## 
##   B   M 
## 357 212</code></pre>
<pre class="r"><code>prop.table(table(df$diagnosis))</code></pre>
<pre><code>## 
##         B         M 
## 0.6274165 0.3725835</code></pre>
<pre class="r"><code># are there any missing values?
any(is.na(df))</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>In total:</p>
<ul>
<li>569 observations
<ul>
<li>357 benign (62%)</li>
<li>212 malignant (37%)</li>
</ul></li>
<li>30 predictor variables</li>
<li>no missing values</li>
</ul>
</div>
</div>
<div id="models" class="section level1">
<h1>Models</h1>
<div id="create-common-validation-indicies-traincontrol" class="section level2">
<h2>Create common validation indicies, <code>trainControl</code></h2>
<p>Start with dividing predictors (features) and response (class) into separated variables <code>df_x</code> and <code>df_y</code>.</p>
<pre class="r"><code># subset predictors variables
df_x &lt;- df %&gt;% select(-diagnosis)

# subset response variable
df_y &lt;- df$diagnosis</code></pre>
<p>In order to achieve fair comparison of models we have to train and test models on the same train/test splits. <code>createFolds</code> seems to be covinient way how to achieve such splits. The outcome (indices) are used later as an <code>index</code> parameter of common <code>trainControl</code> object for all tested models.</p>
<pre class="r"><code># create indices for each fold
my_folds &lt;- createFolds(df_y, k = 10)</code></pre>
<p><code>my_folds</code> now contains indices of held-out (validation) samples of each fold. In addition distribution of class (benign / malignant) is preserved.</p>
<pre class="r"><code># structure of &quot;my_fold&quot;
my_folds %&gt;% glimpse</code></pre>
<pre><code>## List of 10
##  $ Fold01: int [1:57] 7 11 24 27 33 39 42 48 73 76 ...
##  $ Fold02: int [1:57] 4 5 8 13 40 49 61 72 79 99 ...
##  $ Fold03: int [1:57] 25 28 46 58 60 68 82 91 106 107 ...
##  $ Fold04: int [1:57] 2 9 22 23 30 34 59 62 64 80 ...
##  $ Fold05: int [1:56] 1 3 14 21 35 47 57 63 65 75 ...
##  $ Fold06: int [1:57] 6 10 18 36 51 55 74 81 98 101 ...
##  $ Fold07: int [1:57] 16 26 38 45 52 78 85 92 116 125 ...
##  $ Fold08: int [1:57] 19 31 32 43 66 69 70 95 104 105 ...
##  $ Fold09: int [1:57] 15 20 44 53 71 83 86 111 112 126 ...
##  $ Fold10: int [1:57] 12 17 29 37 41 50 54 56 67 87 ...</code></pre>
<pre class="r"><code># distribution of class is preserved in each fold
my_folds %&gt;% 
  map_df(~prop.table(table(df_y[.])))</code></pre>
<pre><code>## # A tibble: 2 x 10
##   Fold01 Fold02 Fold03 Fold04 Fold05 Fold06 Fold07 Fold08 Fold09 Fold10
##    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1  0.632  0.632  0.632  0.614  0.625  0.632  0.632  0.614  0.632  0.632
## 2  0.368  0.368  0.368  0.386  0.375  0.368  0.368  0.386  0.368  0.368</code></pre>
<pre class="r"><code># create trainControl object
my_trainControl &lt;- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = FALSE,
  savePredictions = TRUE,
  index = my_folds)</code></pre>
</div>
<div id="glmnet-model" class="section level2">
<h2>glmnet model</h2>
<pre class="r"><code># train glmnet model
model_glmnet &lt;- train(x = df_x, y = df_y, 
                   method = &quot;glmnet&quot;,
                   metric = &quot;ROC&quot;,
                   tuneLength = 3,
                   trControl = my_trainControl)

# print model
model_glmnet</code></pre>
<pre><code>## glmnet 
## 
## 569 samples
##  30 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (10 reps) 
## Summary of sample sizes: 57, 57, 57, 57, 56, 57, ... 
## Resampling results across tuning parameters:
## 
##   alpha  lambda        ROC        Sens       Spec     
##   0.10   0.0007673665  0.9886822  0.9626787  0.9240424
##   0.10   0.0076736649  0.9895718  0.9689006  0.9224718
##   0.10   0.0767366489  0.9892784  0.9847468  0.9036015
##   0.55   0.0007673665  0.9882370  0.9620528  0.9209066
##   0.55   0.0076736649  0.9885878  0.9704495  0.9235189
##   0.55   0.0767366489  0.9873663  0.9875486  0.8737145
##   1.00   0.0007673665  0.9871038  0.9561435  0.9203748
##   1.00   0.0076736649  0.9881202  0.9695178  0.9203775
##   1.00   0.0767366489  0.9835849  0.9853670  0.8453982
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were alpha = 0.1 and lambda
##  = 0.007673665.</code></pre>
<p>Some remarks:</p>
<ul>
<li>ROC = area under curve</li>
<li>Sens = sensitivity = true positive rate = proportion of positives that are correctly identified as such</li>
<li>Spec = specificity = true negative rate = proportion of negatives that are correctly identified as such</li>
</ul>
<p>Another approach to find the model with the highest ROC:</p>
<pre class="r"><code>model_glmnet$results %&gt;% 
  as.tibble %&gt;%  
  arrange(desc(ROC))</code></pre>
<pre><code>## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.</code></pre>
<pre><code>## # A tibble: 9 x 8
##   alpha   lambda   ROC  Sens  Spec   ROCSD SensSD SpecSD
##   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1  0.1  0.00767  0.990 0.969 0.922 0.00722 0.0276 0.0418
## 2  0.1  0.0767   0.989 0.985 0.904 0.00969 0.0158 0.0451
## 3  0.1  0.000767 0.989 0.963 0.924 0.00601 0.0320 0.0390
## 4  0.55 0.00767  0.989 0.970 0.924 0.00775 0.0222 0.0386
## 5  0.55 0.000767 0.988 0.962 0.921 0.00662 0.0307 0.0417
## 6  1    0.00767  0.988 0.970 0.920 0.00669 0.0261 0.0402
## 7  0.55 0.0767   0.987 0.988 0.874 0.00899 0.0134 0.0382
## 8  1    0.000767 0.987 0.956 0.920 0.00592 0.0367 0.0415
## 9  1    0.0767   0.984 0.985 0.845 0.00977 0.0124 0.0393</code></pre>
<pre class="r"><code># plot ROCs
plot(model_glmnet)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-10-1.svg" width="576" /></p>
<p>By default, the tuning parameters <code>alpha</code> and <code>lamda</code> have three values (<code>tuneLength = 3</code> in <code>train</code> function). Let´s try 20.</p>
<pre class="r"><code># train glmnet model with tuneLength = 10
model_glmnet &lt;- train(x = df_x, y = df_y, 
                   method = &quot;glmnet&quot;,
                   metric = &quot;ROC&quot;,
                   tuneLength = 20,
                   trControl = my_trainControl)</code></pre>
<pre class="r"><code>model_glmnet$results %&gt;% 
  as.tibble %&gt;%  
  arrange(desc(ROC))</code></pre>
<pre><code>## # A tibble: 400 x 8
##    alpha  lambda   ROC  Sens  Spec   ROCSD SensSD SpecSD
##    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1 0.1   0.0230  0.990 0.976 0.920 0.00860 0.0196 0.0431
##  2 0.1   0.0148  0.990 0.972 0.923 0.00808 0.0234 0.0425
##  3 0.1   0.0356  0.990 0.980 0.918 0.00903 0.0155 0.0428
##  4 0.1   0.00956 0.990 0.971 0.924 0.00746 0.0248 0.0422
##  5 0.147 0.0230  0.990 0.976 0.922 0.00873 0.0201 0.0426
##  6 0.147 0.0148  0.990 0.972 0.924 0.00826 0.0241 0.0421
##  7 0.147 0.00956 0.990 0.971 0.923 0.00763 0.0248 0.0418
##  8 0.147 0.0356  0.990 0.981 0.918 0.00917 0.0156 0.0444
##  9 0.195 0.0148  0.990 0.973 0.925 0.00840 0.0216 0.0441
## 10 0.1   0.00616 0.990 0.968 0.924 0.00697 0.0282 0.0399
## # ... with 390 more rows</code></pre>
<p>Tha value ~0.990 seems to be the peak we can get without additional tuning.</p>
<pre class="r"><code># plot ROCs
plot(model_glmnet)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-13-1.svg" width="576" /></p>
</div>
<div id="randomforest-model" class="section level2">
<h2>randomForest model</h2>
<pre class="r"><code># train randomForest model
model_randomForest &lt;- train(x = df_x, y = df_y, 
                            method = &quot;ranger&quot;,
                            metric = &quot;ROC&quot;,
                            importance = &quot;permutation&quot;,
                            tuneLength = 10,
                            trControl = my_trainControl)

# print model
model_randomForest</code></pre>
<pre><code>## Random Forest 
## 
## 569 samples
##  30 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (10 reps) 
## Summary of sample sizes: 57, 57, 57, 57, 56, 57, ... 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   ROC        Sens       Spec     
##    2    gini        0.9863859  0.9694994  0.8988454
##    2    extratrees  0.9884688  0.9794538  0.8941527
##    5    gini        0.9840615  0.9601672  0.8977928
##    5    extratrees  0.9882269  0.9732300  0.8988730
##    8    gini        0.9831402  0.9592326  0.8925572
##    8    extratrees  0.9882908  0.9701196  0.9020198
##   11    gini        0.9817537  0.9583000  0.8883549
##   11    extratrees  0.9880293  0.9701176  0.9067319
##   14    gini        0.9812085  0.9554952  0.8920226
##   14    extratrees  0.9875967  0.9682475  0.9062028
##   17    gini        0.9810775  0.9536309  0.8893993
##   17    extratrees  0.9873329  0.9660726  0.9035712
##   20    gini        0.9801229  0.9520743  0.8920198
##   20    extratrees  0.9874530  0.9651371  0.9061918
##   23    gini        0.9799662  0.9523829  0.8909755
##   23    extratrees  0.9873745  0.9654477  0.9072389
##   26    gini        0.9788685  0.9498945  0.8883494
##   26    extratrees  0.9871659  0.9660717  0.9082860
##   30    gini        0.9797847  0.9492696  0.8914935
##   30    extratrees  0.9869268  0.9654486  0.9061863
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 2, splitrule =
##  extratrees and min.node.size = 1.</code></pre>
<pre class="r"><code># plot model
plot(model_randomForest)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-14-1.svg" width="576" /></p>
<pre class="r"><code>model_randomForest$results %&gt;% 
  as.tibble %&gt;%  
  arrange(desc(ROC))</code></pre>
<pre><code>## # A tibble: 20 x 9
##     mtry min.node.size splitrule    ROC  Sens  Spec   ROCSD SensSD SpecSD
##    &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1     2             1 extratrees 0.988 0.979 0.894 0.00706 0.0179 0.0357
##  2     8             1 extratrees 0.988 0.970 0.902 0.00601 0.0180 0.0438
##  3     5             1 extratrees 0.988 0.973 0.899 0.00668 0.0177 0.0447
##  4    11             1 extratrees 0.988 0.970 0.907 0.00655 0.0183 0.0379
##  5    14             1 extratrees 0.988 0.968 0.906 0.00643 0.0204 0.0415
##  6    20             1 extratrees 0.987 0.965 0.906 0.00636 0.0194 0.0404
##  7    23             1 extratrees 0.987 0.965 0.907 0.00641 0.0199 0.0428
##  8    17             1 extratrees 0.987 0.966 0.904 0.00660 0.0210 0.0433
##  9    26             1 extratrees 0.987 0.966 0.908 0.00679 0.0210 0.0402
## 10    30             1 extratrees 0.987 0.965 0.906 0.00640 0.0194 0.0425
## 11     2             1 gini       0.986 0.969 0.899 0.00477 0.0178 0.0421
## 12     5             1 gini       0.984 0.960 0.898 0.00553 0.0250 0.0436
## 13     8             1 gini       0.983 0.959 0.893 0.00627 0.0253 0.0439
## 14    11             1 gini       0.982 0.958 0.888 0.00665 0.0260 0.0460
## 15    14             1 gini       0.981 0.955 0.892 0.00670 0.0265 0.0402
## 16    17             1 gini       0.981 0.954 0.889 0.00667 0.0268 0.0391
## 17    20             1 gini       0.980 0.952 0.892 0.00769 0.0268 0.0426
## 18    23             1 gini       0.980 0.952 0.891 0.00726 0.0292 0.0406
## 19    30             1 gini       0.980 0.949 0.891 0.00796 0.0270 0.0429
## 20    26             1 gini       0.979 0.950 0.888 0.00867 0.0274 0.0440</code></pre>
<p>Now we can compare top 10 importance variables:</p>
<pre class="r"><code>plot(varImp(model_glmnet, scale = FALSE), top = 10, main = &quot;glmnet&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-16-1.svg" width="576" /></p>
<pre class="r"><code>plot(varImp(model_randomForest, scale = FALSE), top = 10, main = &quot;randomForest&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-16-2.svg" width="576" /></p>
<p>Suprisingly variables quite differs for both models. Any hint why is welcomed:)</p>
</div>
<div id="compare-glmnet-and-randomforest-performance" class="section level2">
<h2>Compare glmnet and randomForest performance</h2>
<pre class="r"><code>model_list &lt;- list(glmnet = model_glmnet, randomForest = model_randomForest)
resamples &lt;- resamples(model_list)

# plot the comparison
bwplot(resamples, metric = &quot;ROC&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-17-1.svg" width="576" /></p>
<pre class="r"><code># plot the comparison per each fold
xyplot(resamples, metric = &quot;ROC&quot;)</code></pre>
<p><img src="/blog/2017-11-10-how-to-model-using-caret-package-cookbook_files/figure-html/unnamed-chunk-17-2.svg" width="576" /></p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<ul>
<li>the glmnet seems to fit the data better than randomForest</li>
<li>even without advanced tuning pretty decent level of accuracy can be achieved</li>
<li>open points: difference between variable importance of both models</li>
</ul>
<p>Thanks for reading. Sure, there is a lot of space for improvements, tweaks. Any kind of feedback is appreciated.</p>
</div>
